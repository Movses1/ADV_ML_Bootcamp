{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9a260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc0153b",
   "metadata": {},
   "source": [
    "### We need to keep track of how many times each word appeared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ce61e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199143, 40535543)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = dict()    # { word:count }\n",
    "total_word_cnt = 0\n",
    "line_num = 0\n",
    "\n",
    "with open('corpus_100k_pass2', 'r', encoding=\"utf8\") as file:\n",
    "    line = file.readline()\n",
    "    \n",
    "    while line:\n",
    "        line_num+=1\n",
    "        if line_num%10000==0:\n",
    "            print('line_num =', line_num)\n",
    "            display.clear_output(wait=True)\n",
    "        \n",
    "        line = line.lower().split()\n",
    "        total_word_cnt += len(line)\n",
    "        \n",
    "        prevs=''\n",
    "        for token in line:                    \n",
    "            if token in vocab:\n",
    "                vocab[token] += 1\n",
    "            else:\n",
    "                vocab[token] = 1\n",
    "                    \n",
    "        line = file.readline()\n",
    "\n",
    "len(vocab), total_word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd3213b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.528218415912185 % appear <=3 times\n"
     ]
    }
   ],
   "source": [
    "cnt_rare = 0\n",
    "num_apear = 3\n",
    "for k,v in vocab.items():\n",
    "    cnt_rare+=(v<=num_apear)\n",
    "    \n",
    "print(cnt_rare/len(vocab)*100, f'% appear <={num_apear} times')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f933ae9",
   "metadata": {},
   "source": [
    "### Lets load the data and remove infrequent tokens and trim the frequent ones\n",
    "### Also remove small sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8da24b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('corpus_100k_pass2', 'r', encoding=\"utf8\")\n",
    "data = f.readlines()\n",
    "for ind in range(len(data)):\n",
    "    data[ind] = data[ind][:-1].split()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c0b0169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# threshold\n",
    "T = 1e-5\n",
    "line_num = 0\n",
    "\n",
    "pop_lines = []\n",
    "for ind in range(len(data)):\n",
    "    line_num+=1\n",
    "    if line_num%10000==0:\n",
    "        print('line_num =', line_num)\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "    pop_indxs = []\n",
    "    for token_ind in range(len(data[ind])):\n",
    "        if vocab[data[ind][token_ind]] <= 3:\n",
    "            pop_indxs.append(token_ind)      # infrequent words will be removed regardless\n",
    "        else:\n",
    "            word_frequency = vocab[data[ind][token_ind]]/total_word_cnt\n",
    "            if  word_frequency > T:\n",
    "                if np.random.rand(1) > np.sqrt(T/word_frequency):\n",
    "                    pop_indxs.append(token_ind)  # save the index for removal based on chance\n",
    "                    \n",
    "    for p in pop_indxs[::-1]:\n",
    "        data[ind].pop(p)      # removing the word\n",
    "\n",
    "    if len(data[ind])<3:\n",
    "        pop_lines.append(ind)\n",
    "\n",
    "for ind, p in enumerate(pop_lines[::-1]):\n",
    "    if ind%1000==0:\n",
    "        print(ind, '/', len(pop_lines))\n",
    "        display.clear_output(wait=True)\n",
    "    data.pop(p)      # removing the line\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59d9b89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42545 4675336\n"
     ]
    }
   ],
   "source": [
    "# just testing\n",
    "\n",
    "cnt_n=0\n",
    "leter_removed = 'ն'\n",
    "for ind in range(len(data)):\n",
    "    for token in data[ind]:\n",
    "        if token == leter_removed:\n",
    "            cnt_n+=1\n",
    "        \n",
    "# new count vs original count\n",
    "print(cnt_n, vocab[leter_removed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae6eab3",
   "metadata": {},
   "source": [
    "#### updating vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dacfe84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72566, 7075674)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets fix our vocabulary\n",
    "\n",
    "vocab = dict()\n",
    "total_word_cnt = 0\n",
    "\n",
    "# adding line sizes\n",
    "line_len = []\n",
    "\n",
    "for line in data:\n",
    "    line_len.append(len(line))\n",
    "    total_word_cnt += len(line)\n",
    "    for token in line:\n",
    "        if token in vocab:\n",
    "            vocab[token]+=1\n",
    "        else:\n",
    "            vocab[token]=1\n",
    "\n",
    "line_len = np.array(line_len, dtype='float32')\n",
    "len(vocab), total_word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6e6bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the negative sampling distribution\n",
    "\n",
    "total_val = 0\n",
    "for k in vocab.keys():\n",
    "    vocab[k] **= 0.75\n",
    "    total_val += vocab[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ba88b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 2962.350185036202, 1629349.650183193, 1629349.650183193)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(vocab.values()), max(vocab.values()), total_val, sum(vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d08de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for quickly choosing negatives\n",
    "data1=''\n",
    "ind = 0\n",
    "for k,v in vocab.items():\n",
    "    data1 += (k+' ')*(round(v))\n",
    "    \n",
    "data1 = data1.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e04c867",
   "metadata": {},
   "source": [
    "### initializing the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a41134a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 40\n",
    "\n",
    "word_embeddings = dict((k, tf.Variable(np.random.normal(0, 0.1, emb_dim))) for k in vocab.keys())\n",
    "output_weights = dict((k, tf.Variable(np.random.normal(0, 0.1, emb_dim))) for k in vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c06c9b",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7aa2c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_model(iter_indx=''):\n",
    "    dump_dict = dict()\n",
    "    ind123=0\n",
    "    for k,v in word_embeddings.items():\n",
    "        ind123+=1\n",
    "        if ind123%1000==0:\n",
    "            print('saving model...', ind123)\n",
    "            display.clear_output(wait=True)\n",
    "        dump_dict[k]=v.numpy().tolist()\n",
    "        \n",
    "    #dump_dict = dict((k, list(v)) for k,v in word_embeddings.items())\n",
    "    json_object = json.dumps(dump_dict, indent=4)\n",
    " \n",
    "    with open(f\"embeddings{iter_indx}.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    print('saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a95cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def sample_negatives(set_context, cnt=10):\n",
    "    negs = []\n",
    "    for _ in range(cnt):\n",
    "        x = np.random.randint(0, len(data1))\n",
    "        while data1[x] in set_context:\n",
    "            x = np.random.randint(0, len(data1))\n",
    "        negs.append(data1[x])\n",
    "    \n",
    "    return np.array(negs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9381a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['բացառապես', 'եզակ', 'ցանկացել', 'առաքելյա', 'մարզաձեւում',\n",
       "       'հեկտար', 'հուշագր', 'ներգրավվմա', 'վերեւ', 'ագ'], dtype='<U11')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_negatives(set(['բարև','մարդ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1553165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word_window(line_indx, word_indx):\n",
    "    """
    "    returns window starting/ending indexes
    "    """
    "\n",
    "    window_start = word_indx - window_size\n",
    "    window_end = word_indx + window_size\n",
    "\n",
    "    # implementing dynamic window\n",
    "    if window_start < 0:\n",
    "        window_end = min(int(line_len[line_indx])-1, window_end-window_start)\n",
    "        window_start = 0\n",
    "    elif window_end >= line_len[line_indx]:\n",
    "        diff_end = window_end - int(line_len[line_indx])\n",
    "        window_start = max(0, window_start-diff_end)\n",
    "        window_end = int(line_len[line_indx]) - 1\n",
    "    \n",
    "    return window_start, window_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e452d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0\n",
      "9.021702741734972\n",
      "9.016181680044058\n",
      "9.014758948446357\n",
      "9.012904144920613\n",
      "9.027002059521262\n",
      "9.021942424447788\n",
      "9.01361950791459\n",
      "9.01501471334074\n",
      "9.021822932884618\n",
      "9.012540820830777\n",
      "9.02184871877059\n",
      "9.019035963060448\n",
      "9.020369214019361\n",
      "9.0243725777299\n"
     ]
    }
   ],
   "source": [
    "window_size = 3 # window from both sides\n",
    "epochs = 5\n",
    "lr = 1e-3 # learning rate\n",
    "negatives_cnt = 12\n",
    "sum_loss = 0\n",
    "loss_print_rate = 1000\n",
    "save_indx_rate = 100000\n",
    "\n",
    "\n",
    "line_probas = line_len/total_word_cnt    # this is done for quick shuffling and sampling\n",
    "line_indxs = []\n",
    "word_indxs = []\n",
    "for i in range(len(line_len)):\n",
    "    for j in range(int(line_len[i])):\n",
    "        line_indxs.append(i)\n",
    "        word_indxs.append(j)\n",
    "line_indxs = np.array(line_indxs)    # will show the line of sampled word\n",
    "word_indxs = np.array(word_indxs)    # will show the index in line of sampled word\n",
    "\n",
    "\n",
    "for _ in range(epochs):\n",
    "    print('epoch -', _)\n",
    "    shuffled_indxs = np.arange(total_word_cnt)\n",
    "    np.random.shuffle(shuffled_indxs)\n",
    "    line_indxs = line_indxs[shuffled_indxs]\n",
    "    word_indxs = word_indxs[shuffled_indxs]\n",
    "    \n",
    "    for word_loop_ind in range(0, total_word_cnt):\n",
    "        if word_loop_ind%loss_print_rate == 0 and word_loop_ind > 0:\n",
    "            print(float(sum_loss)/loss_print_rate)\n",
    "            sum_loss=0\n",
    "        if word_loop_ind%save_indx_rate == 0 and word_loop_ind > 0:\n",
    "            save_model(f'_e{_}_w{word_loop_ind}')\n",
    "            print(f'epoch - {_}, word - {word_loop_ind}')\n",
    "            \n",
    "        \n",
    "        word_indx = word_indxs[word_loop_ind]\n",
    "        line_indx = line_indxs[word_loop_ind]\n",
    "            \n",
    "        window_start, window_end = sample_word_window(line_indx,\n",
    "                                                      word_indx)\n",
    "        \n",
    "        target_word = data[line_indx][word_indx]\n",
    "        window_words = data[line_indx][window_start:window_end+1] # this is for negative sampling\n",
    "\n",
    "        # sampling a single positive and training\n",
    "        i = np.random.randint(window_start, window_end+1)\n",
    "        while i == word_indx:\n",
    "            i = np.random.randint(window_start, window_end+1)\n",
    "            \n",
    "        context_word = data[line_indx][i]\n",
    "        negative_words = sample_negatives(set(window_words), negatives_cnt)\n",
    "\n",
    "        trainable_params = [output_weights[n_w] for n_w in negative_words]\n",
    "        trainable_params.extend([word_embeddings[target_word], output_weights[context_word]])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            p_true = word_embeddings[target_word] * output_weights[context_word]\n",
    "            p_false = [word_embeddings[target_word] * output_weights[n_w] for n_w in negative_words]\n",
    "\n",
    "            sigm_true = tf.math.sigmoid(tf.math.reduce_sum(p_true))\n",
    "            sigm_false = [tf.math.sigmoid(tf.math.reduce_sum(-p_f)) for p_f in p_false]\n",
    "\n",
    "            log_true = tf.math.log(sigm_true)\n",
    "            log_false = [tf.math.log(s_f) for s_f in sigm_false]\n",
    "\n",
    "\n",
    "            loss = -log_true - tf.math.reduce_sum(log_false)\n",
    "            sum_loss += loss\n",
    "            #print(loss)\n",
    "\n",
    "        grads = tape.gradient(loss, trainable_params)\n",
    "\n",
    "        word_embeddings[target_word].assign_sub(grads[-2]*lr)\n",
    "        output_weights[context_word].assign_sub(grads[-1]*lr)\n",
    "\n",
    "        for i in range(len(negative_words)):\n",
    "            output_weights[negative_words[i]].assign_sub(grads[i]*lr)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a02426",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4180c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
