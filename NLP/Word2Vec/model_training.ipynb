{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a844eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8553ef",
   "metadata": {},
   "source": [
    "### We need to keep track of how many times each word appeared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cced730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199143, 40535543)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = dict()    # { word:count }\n",
    "total_word_cnt = 0\n",
    "line_num = 0\n",
    "\n",
    "with open('corpus_100k_pass2', 'r', encoding=\"utf8\") as file:\n",
    "    line = file.readline()\n",
    "    \n",
    "    while line:\n",
    "        line_num+=1\n",
    "        if line_num%10000==0:\n",
    "            print('line_num =', line_num)\n",
    "            display.clear_output(wait=True)\n",
    "        \n",
    "        line = line.lower().split()\n",
    "        total_word_cnt += len(line)\n",
    "        \n",
    "        prevs=''\n",
    "        for token in line:                    \n",
    "            if token in vocab:\n",
    "                vocab[token] += 1\n",
    "            else:\n",
    "                vocab[token] = 1\n",
    "                    \n",
    "        line = file.readline()\n",
    "\n",
    "len(vocab), total_word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2478dc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.528218415912185 % appear <=3 times\n"
     ]
    }
   ],
   "source": [
    "cnt_rare = 0\n",
    "num_apear = 3\n",
    "for k,v in vocab.items():\n",
    "    cnt_rare+=(v<=num_apear)\n",
    "    \n",
    "print(cnt_rare/len(vocab)*100, f'% appear <={num_apear} times')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456d44fc",
   "metadata": {},
   "source": [
    "### Lets load the data and remove infrequent tokens and trim the frequent ones\n",
    "### Also remove small sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cac5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('corpus_100k_pass2', 'r', encoding=\"utf8\")\n",
    "data = f.readlines()\n",
    "for ind in range(len(data)):\n",
    "    data[ind] = data[ind][:-1].split()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "428b31c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# threshold\n",
    "T = 1e-5\n",
    "line_num = 0\n",
    "\n",
    "pop_lines = []\n",
    "for ind in range(len(data)):\n",
    "    line_num+=1\n",
    "    if line_num%10000==0:\n",
    "        print('line_num =', line_num)\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "    pop_indxs = []\n",
    "    for token_ind in range(len(data[ind])):\n",
    "        if vocab[data[ind][token_ind]] <= 3:\n",
    "            pop_indxs.append(token_ind)      # infrequent words will be removed regardless\n",
    "        else:\n",
    "            word_frequency = vocab[data[ind][token_ind]]/total_word_cnt\n",
    "            if  word_frequency > T:\n",
    "                if np.random.rand(1) > np.sqrt(T/word_frequency):\n",
    "                    pop_indxs.append(token_ind)  # save the index for removal based on chance\n",
    "                    \n",
    "    for p in pop_indxs[::-1]:\n",
    "        data[ind].pop(p)      # removing the word\n",
    "\n",
    "    if len(data[ind])<3:\n",
    "        pop_lines.append(ind)\n",
    "\n",
    "for ind, p in enumerate(pop_lines[::-1]):\n",
    "    if ind%1000==0:\n",
    "        print(ind, '/', len(pop_lines))\n",
    "        display.clear_output(wait=True)\n",
    "    data.pop(p)      # removing the line\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b53108d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42545 4675336\n"
     ]
    }
   ],
   "source": [
    "# just testing\n",
    "\n",
    "cnt_n=0\n",
    "leter_removed = 'ն'\n",
    "for ind in range(len(data)):\n",
    "    for token in data[ind]:\n",
    "        if token == leter_removed:\n",
    "            cnt_n+=1\n",
    "        \n",
    "# new count vs original count\n",
    "print(cnt_n, vocab[leter_removed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96daf367",
   "metadata": {},
   "source": [
    "#### updating vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8efe4c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72566, 7075674)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets fix our vocabulary\n",
    "\n",
    "vocab = dict()\n",
    "total_word_cnt = 0\n",
    "\n",
    "# adding line sizes\n",
    "line_len = []\n",
    "\n",
    "for line in data:\n",
    "    line_len.append(len(line))\n",
    "    total_word_cnt += len(line)\n",
    "    for token in line:\n",
    "        if token in vocab:\n",
    "            vocab[token]+=1\n",
    "        else:\n",
    "            vocab[token]=1\n",
    "\n",
    "line_len = np.array(line_len, dtype='float32')\n",
    "len(vocab), total_word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb91f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the negative sampling distribution\n",
    "\n",
    "total_val = 0\n",
    "for k in vocab.keys():\n",
    "    vocab[k] **= 0.75\n",
    "    total_val += vocab[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f393ff6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 2962.350185036202, 1629349.650183193, 1629349.650183193)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(vocab.values()), max(vocab.values()), total_val, sum(vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fcd9b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for quickly choosing negatives\n",
    "data1=''\n",
    "ind = 0\n",
    "for k,v in vocab.items():\n",
    "    data1 += (k+' ')*(round(v))\n",
    "    \n",
    "data1 = data1.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149a2841",
   "metadata": {},
   "source": [
    "### initializing the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5ce98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 40\n",
    "\n",
    "# define new weights or load old ones\n",
    "\n",
    "\"\"\"\n",
    "word_embeddings = dict((k, tf.Variable(np.random.normal(0, 0.1, emb_dim))) for k in vocab.keys())\n",
    "output_weights = dict((k, tf.Variable(np.random.normal(0, 0.1, emb_dim))) for k in vocab.keys())\n",
    "\"\"\"\n",
    "\n",
    "f = open('embeddings_e1_w6000.0K.json')\n",
    "word_embeddings = json.load(f)\n",
    "f.close()\n",
    "f = open('output_weights_e1_w6000K.json')\n",
    "output_weights = json.load(f)\n",
    "f.close()\n",
    "\n",
    "for k in word_embeddings.keys():\n",
    "    word_embeddings[k] = tf.Variable(word_embeddings[k])\n",
    "for k in output_weights.keys():\n",
    "    output_weights[k] = tf.Variable(output_weights[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bd5543",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75b26f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_model(iter_indx=''):\n",
    "    dump_dict = dict()\n",
    "    ind123=0\n",
    "    for k,v in word_embeddings.items():\n",
    "        ind123+=1\n",
    "        if ind123%1000==0:\n",
    "            print('saving model...', ind123)\n",
    "            display.clear_output(wait=True)\n",
    "        dump_dict[k]=v.numpy().tolist()\n",
    "        \n",
    "    #dump_dict = dict((k, list(v)) for k,v in word_embeddings.items())\n",
    "    json_object = json.dumps(dump_dict, indent=4)\n",
    " \n",
    "    with open(f\"embeddings{iter_indx}.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    print('saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfb9e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def sample_negatives(set_context, cnt=10):\n",
    "    negs = []\n",
    "    for _ in range(cnt):\n",
    "        x = np.random.randint(0, len(data1))\n",
    "        while data1[x] in set_context:\n",
    "            x = np.random.randint(0, len(data1))\n",
    "        negs.append(data1[x])\n",
    "    \n",
    "    return np.array(negs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3b5d67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['բացառապես', 'եզակ', 'ցանկացել', 'առաքելյա', 'մարզաձեւում',\n",
       "       'հեկտար', 'հուշագր', 'ներգրավվմա', 'վերեւ', 'ագ'], dtype='<U11')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_negatives(set(['բարև','մարդ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b235d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word_window(line_indx, word_indx):\n",
    "    \"\"\"\n",
    "    returns window starting/ending indexes\n",
    "    \"\"\"\n",
    "    window_start = word_indx - window_size\n",
    "    window_end = word_indx + window_size\n",
    "\n",
    "    # implementing dynamic window\n",
    "    if window_start < 0:\n",
    "        window_end = min(int(line_len[line_indx])-1, window_end-window_start)\n",
    "        window_start = 0\n",
    "    elif window_end >= line_len[line_indx]:\n",
    "        diff_end = window_end - int(line_len[line_indx])\n",
    "        window_start = max(0, window_start-diff_end)\n",
    "        window_end = int(line_len[line_indx]) - 1\n",
    "    \n",
    "    return window_start, window_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf05275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "window_size = 4 # window from both sides\n",
    "epochs = 5\n",
    "lr = 1e-3 # learning rate\n",
    "negatives_cnt = 20\n",
    "sum_loss = 0\n",
    "loss_print_rate = 10000\n",
    "save_indx_rate = 1000000\n",
    "\n",
    "\n",
    "line_probas = line_len/total_word_cnt    # this is done for quick shuffling and sampling\n",
    "line_indxs = []\n",
    "word_indxs = []\n",
    "for i in range(len(line_len)):\n",
    "    for j in range(int(line_len[i])):\n",
    "        line_indxs.append(i)\n",
    "        word_indxs.append(j)\n",
    "line_indxs = np.array(line_indxs)    # will show the line of sampled word\n",
    "word_indxs = np.array(word_indxs)    # will show the index in line of sampled word\n",
    "\n",
    "\n",
    "for _ in range(2, 2+epochs):\n",
    "    print('epoch -', _)\n",
    "    shuffled_indxs = np.arange(total_word_cnt)\n",
    "    np.random.shuffle(shuffled_indxs)\n",
    "    line_indxs = line_indxs[shuffled_indxs]\n",
    "    word_indxs = word_indxs[shuffled_indxs]\n",
    "    \n",
    "    for word_loop_ind in range(0, total_word_cnt):\n",
    "        if word_loop_ind%loss_print_rate == 0 and word_loop_ind > 0:\n",
    "            print(float(sum_loss)/loss_print_rate)\n",
    "            sum_loss=0\n",
    "        if word_loop_ind%save_indx_rate == 0 and word_loop_ind > 0:\n",
    "            save_model(f'_e{_}_w{word_loop_ind//save_indx_rate}M')\n",
    "            print(f'epoch - {_}, word - {word_loop_ind//save_indx_rate}M')\n",
    "            \n",
    "        \n",
    "        word_indx = word_indxs[word_loop_ind]\n",
    "        line_indx = line_indxs[word_loop_ind]\n",
    "            \n",
    "        window_start, window_end = sample_word_window(line_indx,\n",
    "                                                      word_indx)\n",
    "        \n",
    "        target_word = data[line_indx][word_indx]\n",
    "        window_words = data[line_indx][window_start:window_end+1] # this is for negative sampling\n",
    "\n",
    "        # sampling a single positive and training\n",
    "        i = np.random.randint(window_start, window_end+1)\n",
    "        while i == word_indx:\n",
    "            i = np.random.randint(window_start, window_end+1)\n",
    "            \n",
    "        context_word = data[line_indx][i]\n",
    "        negative_words = sample_negatives(set(window_words), negatives_cnt)\n",
    "\n",
    "        trainable_params = [output_weights[n_w] for n_w in negative_words]\n",
    "        trainable_params.extend([word_embeddings[target_word], output_weights[context_word]])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            p_true = word_embeddings[target_word] * output_weights[context_word]\n",
    "            p_false = [word_embeddings[target_word] * output_weights[n_w] for n_w in negative_words]\n",
    "\n",
    "            sigm_true = tf.math.sigmoid(tf.math.reduce_sum(p_true))\n",
    "            sigm_false = [tf.math.sigmoid(tf.math.reduce_sum(-p_f)) for p_f in p_false]\n",
    "\n",
    "            log_true = tf.math.log(sigm_true)\n",
    "            log_false = [tf.math.log(s_f) for s_f in sigm_false]\n",
    "\n",
    "\n",
    "            loss = -log_true - tf.math.reduce_sum(log_false)\n",
    "            sum_loss += loss\n",
    "            #print(loss)\n",
    "\n",
    "        grads = tape.gradient(loss, trainable_params)\n",
    "\n",
    "        word_embeddings[target_word].assign_sub(grads[-2]*lr)\n",
    "        output_weights[context_word].assign_sub(grads[-1]*lr)\n",
    "\n",
    "        for i in range(len(negative_words)):\n",
    "            output_weights[negative_words[i]].assign_sub(grads[i]*lr)\n",
    "                \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98077fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the embeddings\n",
    "\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17b6bbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved!\n"
     ]
    }
   ],
   "source": [
    "# saves the output layer weights\n",
    "\n",
    "dump_dict = dict()\n",
    "ind123=0\n",
    "\n",
    "for k,v in output_weights.items():\n",
    "    ind123+=1\n",
    "    if ind123%1000==0:\n",
    "        print('saving model...', ind123)\n",
    "        display.clear_output(wait=True)\n",
    "    dump_dict[k]=v.numpy().tolist()\n",
    "\n",
    "json_object = json.dumps(dump_dict, indent=4)\n",
    "\n",
    "with open(f\"output_weights_e2_w3M.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "print('saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4ffeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
