{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d24f4e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911235e",
   "metadata": {},
   "source": [
    "### We need to keep track of how many times each word appeared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf51423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242675, 37389589)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = dict()    # { word:count }\n",
    "total_word_cnt = 0\n",
    "line_num = 0\n",
    "\n",
    "with open('corpus_100k_pass2', 'r', encoding=\"utf8\") as file:\n",
    "    line = file.readline()\n",
    "    \n",
    "    while line:\n",
    "        line_num+=1\n",
    "        if line_num%10000==0:\n",
    "            print('line_num =', line_num)\n",
    "            display.clear_output(wait=True)\n",
    "        \n",
    "        line = line.lower().split()\n",
    "        total_word_cnt += len(line)\n",
    "        \n",
    "        prevs=''\n",
    "        for token in line:                    \n",
    "            if token in vocab:\n",
    "                vocab[token] += 1\n",
    "            else:\n",
    "                vocab[token] = 1\n",
    "                    \n",
    "        line = file.readline()\n",
    "\n",
    "len(vocab), total_word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7313d4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.177500772638304 % appear <=3 times\n"
     ]
    }
   ],
   "source": [
    "cnt_rare = 0\n",
    "num_apear = 3\n",
    "for k,v in vocab.items():\n",
    "    cnt_rare+=(v<=num_apear)\n",
    "    \n",
    "print(cnt_rare/len(vocab)*100, f'% appear <={num_apear} times')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebc621e",
   "metadata": {},
   "source": [
    "### Lets load the data and remove infrequent tokens and trim the frequent ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e40ee54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('corpus_100k_pass2', 'r', encoding=\"utf8\")\n",
    "data = f.readlines()\n",
    "for ind in range(len(data)):\n",
    "    data[ind] = data[ind][:-1].split()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cc65bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# threshold\n",
    "T = 1e-5\n",
    "line_num = 0\n",
    "\n",
    "\n",
    "for ind in range(len(data)):\n",
    "    line_num+=1\n",
    "    if line_num%10000==0:\n",
    "        print('line_num =', line_num)\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "    pop_indxs = []\n",
    "    for token_ind in range(len(data[ind])):\n",
    "        if vocab[data[ind][token_ind]] <= 3:\n",
    "            pop_indxs.append(token_ind)      # infrequent words will be removed regardless\n",
    "        else:\n",
    "            word_frequency = vocab[data[ind][token_ind]]/total_word_cnt\n",
    "            if  word_frequency > T:\n",
    "                if np.random.rand(1) > np.sqrt(T/word_frequency):\n",
    "                    pop_indxs.append(token_ind)  # save the index for removal based on chance\n",
    "    for p in pop_indxs[::-1]:\n",
    "        data[ind].pop(p)      # removing the word\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b512617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39481 4241325\n"
     ]
    }
   ],
   "source": [
    "cnt_n=0\n",
    "leter_removed = 'ն'\n",
    "for ind in range(len(data)):\n",
    "    for token in data[ind]:\n",
    "        if token == leter_removed:\n",
    "            cnt_n+=1\n",
    "            \n",
    "print(cnt_n, vocab[leter_removed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f5f980",
   "metadata": {},
   "source": [
    "#### updating vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a00de15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89359, 7817651)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets fix our vocabulary\n",
    "\n",
    "vocab = dict()\n",
    "total_word_cnt = 0\n",
    "\n",
    "# adding line sizes\n",
    "line_len = []\n",
    "\n",
    "for line in data:\n",
    "    line_len.append(len(line))\n",
    "    total_word_cnt += len(line)\n",
    "    for token in line:\n",
    "        if token in vocab:\n",
    "            vocab[token]+=1\n",
    "        else:\n",
    "            vocab[token]=1\n",
    "\n",
    "line_len = np.array(line_len, dtype='float64')\n",
    "len(vocab), total_word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ffd907c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the negative sampling distribution\n",
    "\n",
    "total_val = 0\n",
    "for k in vocab.keys():\n",
    "    vocab[k] **= 0.75\n",
    "    total_val += vocab[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6cce0923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.8284271247461903, 2800.8581094726187, 1883338.191322189, 1883338.191322189)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(vocab.values()), max(vocab.values()), total_val, sum(vocab.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cffe95",
   "metadata": {},
   "source": [
    "### initializing the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f8e6d341",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 40\n",
    "\n",
    "word_embeddings = dict((k, tf.Variable(np.random.normal(0, 0.1, emb_dim))) for k in vocab.keys())\n",
    "output_weights = dict((k, tf.Variable(np.random.normal(0, 0.1, emb_dim))) for k in vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00037ea0",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c3900490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_negatives(set_context, cnt=10):\n",
    "    totl = total_val\n",
    "    for i in set_context:\n",
    "        totl -= vocab[i]\n",
    "    set_usable = set(vocab.keys()) - set_context\n",
    "    usable = list(set_usable)\n",
    "    indxs = np.random.choice(len(usable), cnt, replace=False, p=[vocab[k]/totl for k in usable])\n",
    "    \n",
    "    return np.array(usable)[indxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "617ddbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['բացահայտումը', 'թյունում', 'պայմանները', 'ձեռքբերմանը', 'կունենա',\n",
       "       'վերնագիր', 'կմտածե', 'ինքնազգացող', 'ուղղված', 'հարավայ'],\n",
       "      dtype='<U50')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_negatives(set(['բարև','մարդ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8533be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word_window(line_indx, word_indx):\n",
    "\n",
    "    window_start = word_indx - window_size\n",
    "    window_end = word_indx + window_size\n",
    "\n",
    "    # implementing dynamic window\n",
    "    if window_start < 0:\n",
    "        window_end = min(int(line_len[line_indx])-1, window_end-window_start)\n",
    "        window_start = 0\n",
    "    elif window_end >= line_len[line_indx]:\n",
    "        diff_end = window_end - int(line_len[line_indx])\n",
    "        window_start = max(0, window_start-diff_end)\n",
    "        window_end = int(line_len[line_indx]) - 1\n",
    "    \n",
    "    return window_start, window_end, line_indx, word_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237075e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0\n",
      "18.942756911764373\n",
      "16.685508687315245\n",
      "17.34127083112054\n",
      "16.90375207320818\n",
      "17.372580795409295\n"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "epochs = 5\n",
    "lr = 1e-3 # learning rate\n",
    "loss_print_rate = 100\n",
    "negatives_cnt = 4\n",
    "\n",
    "line_probas = line_len/total_word_cnt\n",
    "line_indxs = []\n",
    "word_indxs = []\n",
    "for i in range(len(line_len)):\n",
    "    for j in range(int(line_len[i])):\n",
    "        line_indxs.append(i)\n",
    "        word_indxs.append(j)\n",
    "line_indxs = np.array(line_indxs)\n",
    "word_indxs = np.array(word_indxs)\n",
    "\n",
    "for _ in range(epochs):\n",
    "    print('epoch -', _)\n",
    "    shuffled_indxs = np.arange(total_word_cnt)\n",
    "    np.random.shuffle(shuffled_indxs)\n",
    "    line_indxs = line_indxs[shuffled_indxs]\n",
    "    word_indxs = word_indxs[shuffled_indxs]\n",
    "    \n",
    "    for word_indx1 in range(0, total_word_cnt):\n",
    "        if word_indx1%loss_print_rate == 0 and word_indx1 > 0:\n",
    "            print(float(sum_loss)/loss_print_rate)\n",
    "            sum_loss=0\n",
    "            \n",
    "            \n",
    "        window_start, window_end, line_indx, word_indx = sample_word_window(line_indxs[word_indx1+b],\n",
    "                                                                            word_indxs[word_indx1+b])\n",
    "        target_word = data[line_indx][word_indx]\n",
    "        window_words = data[line_indx][window_start:window_end+1] # this will be used for negative sampling\n",
    "\n",
    "        for i in range(window_start, window_end+1):\n",
    "            if i != word_indx:\n",
    "                context_word = data[line_indx][word_indx]\n",
    "                negative_words = sample_negatives(set(window_words), negatives_cnt)\n",
    "\n",
    "                trainable_params = []\n",
    "                trainable_params.extend([output_weights[n_w] for n_w in negative_words])\n",
    "                trainable_params.extend([word_embeddings[target_word], output_weights[context_word]])\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    p_true = word_embeddings[target_word] * output_weights[context_word]\n",
    "                    p_false = [word_embeddings[target_word] * output_weights[n_w] for n_w in negative_words]\n",
    "\n",
    "                    sigm_true = tf.math.sigmoid(tf.math.reduce_sum(p_true))\n",
    "                    sigm_false = [tf.math.sigmoid(tf.math.reduce_sum(-p_f)) for p_f in p_false]\n",
    "\n",
    "                    log_true = tf.math.log(sigm_true)\n",
    "                    log_false = [tf.math.log(s_f) for s_f in sigm_false]\n",
    "\n",
    "\n",
    "                    loss = -log_true - tf.math.reduce_sum(log_false)\n",
    "                    sum_loss += loss\n",
    "                    #print(loss)\n",
    "\n",
    "                grads = tape.gradient(loss, trainable_params)\n",
    "                \n",
    "                word_embeddings[target_word].assign_sub(grads[-2]*lr/(window_end-window_start))\n",
    "                output_weights[context_word].assign_sub(grads[-1]*lr)\n",
    "\n",
    "                for i in range(len(negative_words)):\n",
    "                    output_weights[negative_words[i]].assign_sub(grads[i]*lr)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6fb4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
